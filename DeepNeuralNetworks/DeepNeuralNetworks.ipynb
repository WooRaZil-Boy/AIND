{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Deep Neural Networks\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "![dnn_0.png](images/dnn_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons as Logical Operators\n",
    "\n",
    "#### AND\n",
    "![dnn_1.png](images/dnn_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -0.6                    0          Yes\n",
      "      0          1                  -0.1                    0          Yes\n",
      "      1          0                  -0.1                    0          Yes\n",
      "      1          1                   0.4                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.5\n",
    "weight2 = 0.5\n",
    "bias = -0.6\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR\n",
    "![dnn_2.png](images/dnn_2.png)\n",
    "\n",
    "#### NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   0.4                    1          Yes\n",
      "      0          1                  -0.1                    0          Yes\n",
      "      1          0                   0.4                    1          Yes\n",
      "      1          1                  -0.1                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -0.5\n",
    "bias = 0.4\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR\n",
    "![dnn_3.png](images/dnn_3.png)\n",
    "![dnn_4.png](images/dnn_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Trick\n",
    "\n",
    "무작위로 선을 긋고, learning rate로 조금씩 이동시켜 적절한 구분선을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete vs Continuous\n",
    "\n",
    "![dnn_5.png](images/dnn_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "exp로 항상 양수로 만든다.\n",
    "\n",
    "![dnn_6.png](images/dnn_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "\n",
    "손실 함수. -ln (자연로그)를 씌워 구한다. 크로스 엔트로피가 낮을 수록 좋은 모델    \n",
    "Goal : Minimize the Cross Entropy\n",
    "\n",
    "![dnn_7.png](images/dnn_7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    \n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "![dnn_8.png](images/dnn_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def prediction(X, W, b):\n",
    "    return sigmoid(np.matmul(X,W)+b)\n",
    "def error_vector(y, y_hat):\n",
    "    return [-y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i]) for i in range(len(y))]\n",
    "def error(y, y_hat):\n",
    "    ev = error_vector(y, y_hat)\n",
    "    return sum(ev)/len(ev)\n",
    "\n",
    "# TODO: Fill in the code below to calculate the gradient of the error function.\n",
    "# The result should be a list of three lists:\n",
    "# The first list should contain the gradient (partial derivatives) with respect to w1\n",
    "# The second list should contain the gradient (partial derivatives) with respect to w2\n",
    "# The third list should contain the gradient (partial derivatives) with respect to b\n",
    "def dErrors(X, y, y_hat):\n",
    "    DErrorsDx1 = [X[i][0]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    DErrorsDx2 = [X[i][1]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    DErrorsDb = [y[i]-y_hat[i] for i in range(len(y))]\n",
    "    return DErrorsDx1, DErrorsDx2, DErrorsDb\n",
    "\n",
    "# TODO: Fill in the code below to implement the gradient descent step.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b.\n",
    "# It should calculate the prediction, the gradients, and use them to\n",
    "# update the weights and bias W, b. Then return W and b.\n",
    "# The error e will be calculated and returned for you, for plotting purposes.\n",
    "def gradientDescentStep(X, y, W, b, learn_rate = 0.01):\n",
    "    y_hat = prediction(X,W,b)\n",
    "    errors = error_vector(y, y_hat)\n",
    "    derivErrors = dErrors(X, y, y_hat)\n",
    "    W[0] += sum(derivErrors[0])*learn_rate\n",
    "    W[1] += sum(derivErrors[1])*learn_rate\n",
    "    b += sum(derivErrors[2])*learn_rate\n",
    "    return W, b, sum(errors)\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainLR(X, y, learn_rate = 0.01, num_epochs = 100):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    # Initialize the weights randomly\n",
    "    W = np.array(np.random.rand(2,1))*2 -1\n",
    "    b = np.random.rand(1)[0]*2 - 1\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    errors = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the gradient descent step.\n",
    "        W, b, error = gradientDescentStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "        errors.append(error)\n",
    "    return boundary_lines, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "\n",
    "실생활에서는 Linear 모델이 아닌 경우가 많다. 퍼셉트론을 연결한 뉴런 네트워크로 Non-Linear Model을 구현할 수 있다. \n",
    "\n",
    "![dnn_9.png](images/dnn_9.png)\n",
    "![dnn_10.png](images/dnn_10.png)\n",
    "\n",
    "Input layer, Hidden layer, Output layer로 이루어진다. Output layer가 여러 개 있어, classification하는 모델도 만들 수 있다. 이 경우에는 soft-max를 활성함수로 주로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential() #선형 model 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# X has shape (num_rows, num_cols), where the training data are stored\n",
    "# as row vectors\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32) #인풋\n",
    "\n",
    "# y must have an output vector for each input vector\n",
    "y = np.array([[0], [0], [0], [1]], dtype=np.float32) #정답레이블\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential() #Sequential 모델 생성\n",
    "\n",
    "# 1st Layer - Add an input layer of 32 nodes with the same input shape as\n",
    "# the training samples in X\n",
    "model.add(Dense(32, input_dim=X.shape[1])) #첫 레이어에는 input shape를 지정해 줘야 하지만 다른 레이어에서는 알아서 추정한다.\n",
    "#입력으로 2개의 요소를 가진 벡터를 받는 32개의 노드 생성 #히든 레이어\n",
    "\n",
    "# Add a softmax activation layer\n",
    "model.add(Activation('softmax')) #활성 함수로 소프트 맥스\n",
    "#model.add(Dense(128, activation=\"softmax\")) 이런식으로 한 번에 추가해 줄 수도 있다.\n",
    "\n",
    "# 2nd Layer - Add a fully connected output layer\n",
    "model.add(Dense(1)) #아웃풋 레이어. 노드가 1개\n",
    "\n",
    "# Add a sigmoid activation layer\n",
    "model.add(Activation('sigmoid')) #분류에서 아웃풋의 활성화함수는 시그모이드로 하는 것이 일반적이다.\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "# #모델 빌드 후 실행 전에 컴파일 해야 한다. 백엔드(tensorflow 등)를 호출하고 최적화, 손실 함수 등의 매개변수를 입력할 수 있다.\n",
    "# #손실함수로 categorical_crossentropy : 클래스가 두 개인 경우에만 사용\n",
    "# #최적화 함수 : adam #모델 평가 측정 항목 : 정확도\n",
    "\n",
    "# model.summary() #모델의 결과를 볼 수 있다.\n",
    "\n",
    "# model.fit(X, y, nb_epoch=1000, verbose=0) #모델 학습\n",
    "\n",
    "# model.evaluate() #모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s\n",
      "\n",
      "Accuracy:  0.5\n",
      "\n",
      "Predictions:\n",
      "4/4 [==============================] - 0s\n",
      "[[ 0.51922798]\n",
      " [ 0.54183304]\n",
      " [ 0.46536183]\n",
      " [ 0.47800067]]\n"
     ]
    }
   ],
   "source": [
    "#XOR 예제\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Using TensorFlow 1.0.0; use tf.python_io in later versions\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "\n",
    "# Add required layers\n",
    "xor.add(Dense(8, input_dim=X.shape[1]))\n",
    "xor.add(Activation(\"tanh\"))\n",
    "xor.add(Dense(1))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "xor.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "# xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=50, verbose=0) #nb_epoch이 늘어날 수록 시행 수가 늘어나 정확도 증가\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs Stochastic Gradient Descent\n",
    "\n",
    "배치를 나눠서 업데이트를 반복한다. 예를 들어 20개의 데이터가 있으면 4개씩 5배치로 나눠서 시행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "![dnn_11.png](images/dnn_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "![dnn_12.png](images/dnn_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "epoch마다 몇 개의 특정 노드를 제외하고 결과를 도출한다. 단, 모델 트레이닝이 아닌 실제 예측에서는 모든 노드를 포함해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.2)) #드롭 아웃 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
